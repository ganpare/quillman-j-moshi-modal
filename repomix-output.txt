This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-08T14:00:24.723Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.github/actions/setup/action.yml
.github/workflows/cd.yml
.github/workflows/e2e.yml
.gitignore
.isort.cfg
.pre-commit-config.yaml
code_summary.md
dependency_diagram.md
LICENSE
modal_guide.md
README_DOCS.md
README.md
requirements/requirements-dev.txt
requirements/requirements.txt
src/__init__.py
src/app.py
src/common.py
src/frontend/app.jsx
src/frontend/index.html
src/frontend/modal-logo.svg
src/moshi.py
tests/e2e_test.py
tests/moshi_client.py

================================================================
Files
================================================================

================
File: .github/actions/setup/action.yml
================
name: setup

description: Set up a Python environment.

inputs:
  version:
    description: Which Python version to install
    required: false
    default: "3.11"

runs:
  using: composite
  steps:
    - name: Install Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ inputs.version }}

    - name: Install project requirements
      shell: bash
      run: pip install -r requirements/requirements.txt

================
File: .github/workflows/cd.yml
================
name: Deploy
on:
  push:
    branches:
      - main
  workflow_dispatch:
  schedule:
    - cron: "17 9 * * *"

jobs:
  deploy:
    name: deploy
    runs-on: ubuntu-20.04
    env:
      MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
      MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
      MODAL_ENVIRONMENT: main
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1
      - uses: ./.github/actions/setup
      - name: Deploy
        run: |
          modal token set --token-id $MODAL_TOKEN_ID --token-secret $MODAL_TOKEN_SECRET
          modal deploy src.app

================
File: .github/workflows/e2e.yml
================
name: End-to-end Test
on:
  push:
    branches:
      - main
  pull_request:
  workflow_dispatch:
  schedule:
    - cron: "17 9 * * *"

jobs:
  e2etest:
    name: e2etest
    runs-on: ubuntu-20.04
    env:
      MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
      MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}
      MODAL_ENVIRONMENT: main
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 1
      - uses: ./.github/actions/setup
      - name: Run testing script
        run: |
          pip install -r requirements/requirements-dev.txt
          modal token set --token-id $MODAL_TOKEN_ID --token-secret $MODAL_TOKEN_SECRET
          modal serve src.app &
          python tests/e2e_test.py

================
File: .gitignore
================
**/__pycache__
venv/
.venv

================
File: .isort.cfg
================
[settings]
known_first_party=modal
extra_standard_library=pytest
profile=black

================
File: .pre-commit-config.yaml
================
repos:
  - repo: https://github.com/hadialqattan/pycln
    rev: v1.1.0
    hooks:
      - id: pycln
        args: ["-a"]
  - repo: https://github.com/pycqa/isort
    rev: 5.11.5
    hooks:
      - id: isort
        name: isort (python)
        args: ["--filter-files"]
  - repo: https://github.com/ambv/black
    rev: 22.3.0
    hooks:
      - id: black
        language_version: python3
  - repo: https://github.com/pre-commit/mirrors-prettier
    rev: v2.6.2
    hooks:
      - id: prettier

================
File: code_summary.md
================
# QuillMan コード解析サマリー

## 1. プロジェクト構成

### ディレクトリ構造
src/ ├── app.py # フロントエンドの静的ファイル配信用FastAPIサーバー ├── common.py # 共通のModal Appインスタンス定義 ├── moshi.py # Moshi音声対話モデルのwebsocketサーバー └── frontend/ # フロントエンドのReactアプリケーション ├── app.jsx # メインのReactコンポーネント └── index.html # エントリーポイントHTML


## 2. バックエンド実装

### common.py
- `modal.App`インスタンスを作成し、"quillman"という名前で設定
- 他のモジュールで共有される中心的なAppインスタンスを提供

### moshi.py
主要なバックエンド実装を含むモジュール

#### 環境設定
- Modal Image設定
  - Python 3.11のDebian Slim
  - 必要なパッケージ: moshi, fastapi, huggingface_hub, hf_transfer, sphn
  - HuggingFace転送の有効化

#### Moshiクラス
- GPUインスタンス管理と音声対話処理を担当
- デコレータ:
  - `@app.cls`: A10G GPU使用、300秒のアイドルタイムアウト
  - `@modal.build`: モデルのダウンロード
  - `@modal.enter`: モデルの初期化とGPUウォームアップ

主要なメソッド：
1. `download_model()`: 必要なモデルファイルをダウンロード
2. `enter()`: 
   - GPUデバイスの設定
   - Mimiモデルの初期化（音声エンコーダー/デコーダー）
   - Moshiモデルの初期化（言語モデル）
   - テキストトークナイザーの設定
   - GPUのウォームアップ処理
3. `reset_state()`: 
   - Opusストリームの初期化
   - チャット履歴のリセット
4. `web()`: WebSocket通信を処理するFastAPIアプリケーション

WebSocket処理の特徴：
- 3つの非同期ループを使用:
  1. `recv_loop()`: クライアントからの音声データ受信
  2. `inference_loop()`: 音声認識と応答生成
  3. `send_loop()`: 生成された音声の送信
- 双方向リアルタイム通信の実現
- Opusフォーマットによる効率的な音声データ転送

### app.py
フロントエンド配信用のFastAPIサーバー

特徴：
- 静的ファイルの配信設定
- CORSミドルウェアの設定
- キャッシュ無効化の実装
- 同時接続100までの対応

## 3. フロントエンド実装

### index.html
- 必要なライブラリの読み込み:
  - React
  - TailwindCSS
  - Opus関連（録音・デコード用）
- カスタムカラーテーマの設定

### app.jsx
Reactを使用したSPAの実装

#### 主要コンポーネント
1. `App`: メインコンポーネント
   - 状態管理: マイク入力、音声再生、WebSocket接続
   - 音声処理の初期化と管理
   - UIレイアウトの構成

2. `AudioControl`: マイク制御UI
   - 音声入力の可視化
   - ミュート機能の提供

3. `TextOutput`: テキスト出力表示
   - 音声認識結果の表示
   - 自動スクロール機能

#### 主要機能
1. 音声入力処理:
   - OpusRecorderを使用したマイク録音
   - リアルタイム音量表示
   - 録音ゲイン制御

2. 音声出力処理:
   - Opus形式の音声データをPCMにデコード
   - Web Audio APIによるシームレスな音声再生
   - バッファリングによる途切れのない再生

3. WebSocket通信:
   - バイナリデータの送受信
   - 音声データとテキストデータの分離処理
   - 接続状態の管理

## 4. 外部依存関係

### Python パッケージ
- modal: サーバーレスコンピューティング基盤
- fastapi: Webアプリケーションフレームワーク
- moshi: 音声対話モデル
- huggingface_hub: モデルファイル管理
- sphn: 音声処理ユーティリティ
- torch: 深層学習フレームワーク
- sentencepiece: テキストトークナイザー

### JavaScript ライブラリ
- React: UIフレームワーク
- Opus Recorder: 音声録音・エンコード
- Ogg Opus Decoder: 音声デコード
- TailwindCSS: スタイリング

## 5. 重要なデータ構造

### 音声データフロー
1. クライアント側:
マイク入力 → PCM → Opus エンコード → WebSocket → サーバー


2. サーバー側:
Opus → PCM → Mimi エンコード → Moshi 処理 → Mimi デコード → Opus → WebSocket → クライアント


### モデル状態管理
- チャット履歴: LMGenクラスで管理
- 音声バッファ: OpusStream形式で管理
- セッション状態: WebSocketコネクションごとに初期化

================
File: dependency_diagram.md
================
# QuillMan 依存関係図

```mermaid
graph TD
    subgraph "フロントエンド"
        F_INDEX[index.html]
        F_APP[app.jsx]
        F_INDEX --> F_APP
        
        subgraph "外部ライブラリ"
            REACT[React]
            TAILWIND[TailwindCSS]
            OPUS_REC[Opus Recorder]
            OPUS_DEC[Opus Decoder]
            F_APP --> REACT
            F_APP --> TAILWIND
            F_APP --> OPUS_REC
            F_APP --> OPUS_DEC
        end
    end

    subgraph "バックエンド"
        B_COMMON[common.py]
        B_APP[app.py]
        B_MOSHI[moshi.py]
        
        B_COMMON -->|Modal App| B_APP
        B_COMMON -->|Modal App| B_MOSHI
        B_APP -->|imports| B_MOSHI

        subgraph "外部パッケージ"
            MODAL[Modal]
            FASTAPI[FastAPI]
            MOSHI_PKG[Moshi]
            HF[HuggingFace Hub]
            TORCH[PyTorch]
            SPHN[SPHN]
            
            B_COMMON --> MODAL
            B_APP --> FASTAPI
            B_MOSHI --> MOSHI_PKG
            B_MOSHI --> HF
            B_MOSHI --> TORCH
            B_MOSHI --> SPHN
        end
    end

    subgraph "通信"
        WS{WebSocket}
        STATIC{Static Files}
        
        F_APP -->|Audio Data| WS
        WS -->|Audio/Text| F_APP
        B_MOSHI -->|Handles| WS
        B_APP -->|Serves| STATIC
        STATIC -->|Loads| F_INDEX
    end

    subgraph "モデル"
        MIMI[Mimi Model]
        MOSHI_MODEL[Moshi Model]
        TEXT_TOK[Text Tokenizer]
        
        B_MOSHI --> MIMI
        B_MOSHI --> MOSHI_MODEL
        B_MOSHI --> TEXT_TOK
    end

================
File: LICENSE
================
MIT License

Copyright (c) 2024 Modal Labs

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: modal_guide.md
================
# プロジェクト解説

## 1. プロジェクト構成

```
quillman/
├── src/
│   ├── app.py         # メインのWebアプリケーション
│   ├── moshi.py       # 音声処理とLLMの核となるロジック
│   ├── common.py      # Modal appの共通設定
│   └── frontend/      # フロントエンドのアセット
└── requirements/      # 依存関係の管理

```

## 2. Modalの利用について

### ローカル環境とModal環境の違い

1. **ローカル環境**
   - 開発時に使用する環境
   - `conda activate modal`で専用の環境をアクティベート
   - モデルや依存関係のテスト、コード編集を行う
   - `modal serve`でローカルでの動作確認が可能

2. **Modal環境**
   - クラウドで実行される本番環境
   - GPUリソース（A10G）を使用
   - 依存関係は`modal.Image`で管理
   - モデルは`@modal.build()`で自動的にダウンロード

### Modalの主要コンポーネント

1. **アプリケーション定義**
```python
# common.py
app = App(name="quillman")
```

2. **イメージの設定（moshi.py）**
```python
image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "moshi==0.1.0",
        "fastapi==0.115.5",
        "huggingface_hub==0.24.7",
        # その他の依存関係
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
)
```

3. **モデルの管理**
```python
@modal.build()  # モデルのダウンロードを制御
def download_model(self):
    hf_hub_download(loaders.DEFAULT_REPO, loaders.MOSHI_NAME)
    # その他のモデルファイル
```

4. **Webアプリケーション（app.py）**
```python
@app.function(
    mounts=[modal.Mount.from_local_dir(static_path, remote_path="/assets")],
    container_idle_timeout=600,
    timeout=600,
    allow_concurrent_inputs=100,
    image=modal.Image.debian_slim(python_version="3.11").pip_install(
        "fastapi==0.115.5"
    ),
)
@modal.asgi_app()
def web():
    # FastAPIアプリケーションの定義
```

## 3. 開発からデプロイまでのフロー

1. **ローカル開発**
   ```bash
   conda activate modal  # Modal用の環境をアクティベート
   modal serve src.app  # ローカルでの動作確認
   ```

2. **デプロイ**
   ```bash
   conda activate modal
   modal deploy src.app
   ```

3. **デプロイ後のエンドポイント**
   - フロントエンド: https://ganpare--quillman-web.modal.run
   - WebSocket: https://ganpare--quillman-moshi-web.modal.run

## 4. 重要なポイント

1. **依存関係の管理**
   - ローカル環境: `requirements.txt`でmodalパッケージのみ管理
   - Modal環境: `modal.Image`内で必要なパッケージを定義

2. **リソース管理**
   - GPU: Modalで自動的にA10Gを割り当て
   - ストレージ: モデルファイルは`@modal.build()`で自動管理
   - スケーリング: `allow_concurrent_inputs=100`で同時接続を制御

3. **デバッグとモニタリング**
   - ローカル環境: `modal serve`でログ確認
   - Modal環境: Modalのダッシュボードでモニタリング

================
File: README_DOCS.md
================
# QuiLLMan: Voice Chat with Moshi

[QuiLLMan](https://github.com/modal-labs/quillman) is a complete voice chat application built on Modal: you speak and the chatbot speaks back!

At the core is Kyutai Lab's [Moshi](https://github.com/kyutai-labs/moshi) model, a speech-to-speech language model that will continuously listen, plan, and respond to the user.

Thanks to bidirectional websocket streaming and [Opus audio compression](https://opus-codec.org/), response times on good internet can be nearly instantaneous, closely matching the cadence of human speech.

You can find the demo live [here](https://modal-labs--quillman-web.modal.run/).

![Quillman](https://github.com/user-attachments/assets/afda5874-8509-4f56-9f25-d734b8f1c40a)

Everything — from the React frontend to the model backend — is deployed serverlessly on Modal, allowing it to automatically scale and ensuring you only pay for the compute you use.

This page provides a high-level walkthrough of the [GitHub repo](https://github.com/modal-labs/quillman).

## Code overview

Traditionally, building a bidirectional streaming web application as compute-heavy as QuiLLMan would take a lot of work, and it's especially difficult to make it robust and scale to handle many concurrent users.

But with Modal, it’s as simple as writing two different classes and running a CLI command.

Our project structure looks like this:

1. [Moshi Websocket Server](https://modal.com/docs/examples/llm-voice-chat#moshi-websocket-server): loads an instance of the Moshi model and maintains a bidirectional websocket connection with the client.
2. [React Frontend](https://modal.com/docs/examples/llm-voice-chat#react-frontend): runs client-side interaction logic.

Let’s go through each of these components in more detail.

### FastAPI Server

Both frontend and backend are served via a [FastAPI Server](https://fastapi.tiangolo.com/), which is a popular Python web framework for building REST APIs.

On Modal, a function or class method can be exposed as a web endpoint by decorating it with [`@app.asgi_app()`](https://modal.com/docs/reference/modal.asgi_app#modalasgi_app) and returning a FastAPI app. You're then free to configure the FastAPI server however you like, including adding middleware, serving static files, and running websockets.

### Moshi Websocket Server

Traditionally, a speech-to-speech chat app requires three distinct modules: speech-to-text, text-to-text, and text-to-speech. Passing data between these modules introduces bottlenecks, and can limit the speed of the app and forces a turn-by-turn conversation which can feel unnatural.

Kyutai Lab's [Moshi](https://github.com/kyutai-labs/moshi) bundles all modalities into one model, which decreases latency and makes for a much simpler app.

Under the hood, Moshi uses the [Mimi](https://huggingface.co/kyutai/mimi) streaming encoder/decoder model to maintain an unbroken stream of audio in and out. The encoded audio is processed by a [speech-text foundation model](https://huggingface.co/kyutai/moshiko-pytorch-bf16), which uses an internal monologue to determine when and how to respond.

Using a streaming model introduces a few challenges not normally seen in inference backends:

1. The model is _stateful_, meaning it maintains context of the conversation so far. This means a model instance cannot be shared between user conversations, so we must run a unique GPU per user session, which is normally not an easy feat!
2. The model is _streaming_, so the interface around it is not as simple as a POST request. We must find a way to stream audio data in and out, and do it fast enough for seamless playback.

We solve both of these in `src/moshi.py`, using a few Modal features.

To solve statefulness, we just spin up a new GPU per concurrent user.
That's easy with Modal!

```python
@app.cls(
    image=image,
    gpu="A10G",
    container_idle_timeout=300,
    ...
)
class Moshi:
    # ...
```

With this setting, if a new user connects, a new GPU instance is created! When any user disconnects, the state of their model is reset and that GPU instance is returned to the warm pool for re-use (for up to 300 seconds). Be aware that a GPU per user is not going to be cheap, but it's the simplest way to ensure user sessions are isolated.

For streaming, we use FastAPI's support for bidirectional websockets. This allows clients to establish a single connection at the start of their session, and stream audio data both ways.

Just as a FastAPI server can run from a Modal function, it can also be attached to a Modal class method, allowing us to couple a prewarmed Moshi model to a websocket session.

```python
    @modal.asgi_app()
    def web(self):
        from fastapi import FastAPI, Response, WebSocket, WebSocketDisconnect

        web_app = FastAPI()
        @web_app.websocket("/ws")
        async def websocket(ws: WebSocket):
            with torch.no_grad():
                await ws.accept()

                # handle user session

                # spawn loops for async IO
                async def recv_loop():
                    while True:
                        data = await ws.receive_bytes()
                        # send data into inference stream...

                async def send_loop():
                    while True:
                        await asyncio.sleep(0.001)
                        msg = self.opus_stream_outbound.read_bytes()
                        # send inference output to user ...
```

To run a [development server](https://modal.com/docs/guide/webhooks#developing-with-modal-serve) for the Moshi module, run this command from the root of the repo.

```shell
modal serve src.moshi
```

In the terminal output, you'll find a URL for creating a websocket connection.

### React Frontend

The frontend is a static React app, found in the `src/frontend` directory and served by `src/app.py`.

We use the [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API) to record audio from the user's microphone and playback audio responses from the model.

For efficient audio transmission, we use the [Opus codec](https://opus-codec.org/) to compress audio across the network. Opus recording and playback are supported by the [`opus-recorder`](https://github.com/chris-rudmin/opus-recorder) and [`ogg-opus-decoder`](https://github.com/eshaz/wasm-audio-decoders/tree/master/src/ogg-opus-decoder) libraries.

To serve the frontend assets, run this command from the root of the repo.

```shell
modal serve src.app
```

Since `src/app.py` imports the `src/moshi.py` module, this `serve` command also serves the Moshi websocket server as its own endpoint.

## Deploy

When you're ready to go live, use the `deploy` command to deploy the app to Modal.

```shell
modal deploy src.app
```

## Steal this example

The code for this entire example is [available on GitHub](https://github.com/modal-labs/quillman), so feel free to fork it and make it your own!

================
File: README.md
================
# QuiLLMan: Voice Chat with Moshi

A complete voice chat app powered by a speech-to-speech language model and bidirectional streaming.

On the backend is Kyutai Lab's [Moshi](https://github.com/kyutai-labs/moshi) model, which will continuously listen, plan, and respond to the user. It uses the [Mimi](https://huggingface.co/kyutai/mimi) streaming encoder/decoder model to maintain an unbroken stream of audio in and out, and a [speech-text foundation model](https://huggingface.co/kyutai/moshiko-pytorch-bf16) to determine when and how to respond.

Thanks to bidirectional websocket streaming and use of the [Opus audio codec](https://opus-codec.org/) for compressing audio across the network, response times on good internet can be nearly instantaneous, closely matching the cadence of human speech.

You can find the demo live [here](https://modal-labs--quillman-web.modal.run/).

![Quillman](https://github.com/user-attachments/assets/afda5874-8509-4f56-9f25-d734b8f1c40a)

This repo is meant to serve as a starting point for your own language model-based apps, as well as a playground for experimentation. Contributions are welcome and encouraged!

[Note: this code is provided for illustration only; please remember to check the license before using any model for commercial purposes.]

## File structure

1. React frontend ([`src/frontend/`](./src/frontend/)), served by [`src/app.py`](./src/app.py)
2. Moshi websocket server ([`src/moshi.py`](./src/moshi.py))

## Developing locally

### Requirements

- `modal` installed in your current Python virtual environment (`pip install modal`)
- A [Modal](http://modal.com/) account (`modal setup`)
- A Modal token set up in your environment (`modal token new`)

### Developing the inference module

The Moshi server is a [Modal class](https://modal.com/docs/reference/modal.Cls#modalcls) module to load the models and maintain streaming state, with a [FastAPI](https://fastapi.tiangolo.com/) http server to expose a websocket interface over the internet.

To run a [development server]((https://modal.com/docs/guide/webhooks#developing-with-modal-serve)) for the Moshi module, run this command from the root of the repo.

```shell
modal serve src.moshi
```

In the terminal output, you'll find a URL for creating a websocket connection.

While the `modal serve` process is running, changes to any of the project files will be automatically applied. `Ctrl+C` will stop the app. 

### Testing the websocket connection
From a seperate terminal, we can test the websocket connection directly from the command line with the `tests/moshi_client.py` client.

It requires non-standard dependencies, which can be installed with:
```shell
python -m venv venv
source venv/bin/activate
pip install -r requirements/requirements-dev.txt
```

With dependencies installed, run the terminal client with:
```shell
python tests/moshi_client.py
```

And begin speaking! Be sure to have your microphone and speakers enabled.

### Developing the http server and frontend

The http server at `src/app.py` is a second [FastAPI](https://fastapi.tiangolo.com/) app, for serving the frontend as static files.

A [development server]((https://modal.com/docs/guide/webhooks#developing-with-modal-serve)) can be run with:

```shell
modal serve src.app
```

Since `src/app.py` imports the `src/moshi.py` module, this also starts the Moshi websocket server.

In the terminal output, you'll find a URL that you can visit to use your app. 
While the `modal serve` process is running, changes to any of the project files will be automatically applied. `Ctrl+C` will stop the app. 

Note that for frontend changes, the browser cache may need to be cleared.

### Deploying to Modal

Once you're happy with your changes, [deploy](https://modal.com/docs/guide/managing-deployments#creating-deployments) your app:

```shell
modal deploy src.app
```

This will deploy both the frontend server and the Moshi websocket server.

Note that leaving the app deployed on Modal doesn't cost you anything! Modal apps are serverless and scale to 0 when not in use.

================
File: requirements/requirements-dev.txt
================
-r requirements.txt
sounddevice
numpy
sphn
aiohttp

================
File: requirements/requirements.txt
================
modal
# that's it :)

================
File: src/__init__.py
================
# Initialize the package

================
File: src/app.py
================
"""
Main web application service. Serves the static frontend.
"""
from pathlib import Path
import modal
from .moshi import Moshi  # makes modal deploy also deploy moshi

from .common import app

static_path = Path(__file__).with_name("frontend").resolve()


@app.function(
    mounts=[modal.Mount.from_local_dir(static_path, remote_path="/assets")],
    container_idle_timeout=600,
    timeout=600,
    allow_concurrent_inputs=100,
    image=modal.Image.debian_slim(python_version="3.11").pip_install(
        "fastapi==0.115.5"
    ),
)
@modal.asgi_app()
def web():
    from fastapi import FastAPI
    from fastapi.middleware.cors import CORSMiddleware
    from fastapi.staticfiles import StaticFiles

    # disable caching on static files
    StaticFiles.is_not_modified = lambda self, *args, **kwargs: False

    web_app = FastAPI()

    web_app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Serve static files, for the frontend
    web_app.mount("/", StaticFiles(directory="/assets", html=True))
    return web_app

================
File: src/common.py
================
from modal import App

app = App(name="quillman")

================
File: src/frontend/app.jsx
================
const { useRef, useEffect, useState } = React;

const baseURL = "" // points to whatever is serving this app (eg your -dev.modal.run for modal serve, or .modal.run for modal deploy)

const getBaseURL = () => {
  // use current web app server domain to construct the url for the moshi app
  const currentURL = new URL(window.location.href);
  let hostname = currentURL.hostname;
  hostname = hostname.replace('-web', '-moshi-web');
  const wsProtocol = currentURL.protocol === 'https:' ? 'wss:' : 'ws:';
  return `${wsProtocol}//${hostname}/ws`; 
}

const App = () => {
  // Mic Input
  const [recorder, setRecorder] = useState(null); // Opus recorder
  const [amplitude, setAmplitude] = useState(0); // Amplitude, captured from PCM analyzer

  // Audio playback
  const [audioContext] = useState(() => new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 48000 }));
  const sourceNodeRef = useRef(null); // Audio source node
  const scheduledEndTimeRef = useRef(0); // Scheduled end time for audio playback
  const decoderRef = useRef(null); // Decoder for converting opus to PCM

  // WebSocket
  const socketRef = useRef(null); // Ongoing websocket connection

  // UI State
  const [warmupComplete, setWarmupComplete] = useState(false);
  const [completedSentences, setCompletedSentences] = useState([]);
  const [pendingSentence, setPendingSentence] = useState('');


  // Mic Input: start the Opus recorder
  const startRecording = async () => {
    // prompts user for permission to use microphone
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

    const recorder = new Recorder({
      encoderPath: "https://cdn.jsdelivr.net/npm/opus-recorder@latest/dist/encoderWorker.min.js",
      streamPages: true,
      encoderApplication: 2049,
      encoderFrameSize: 80, // milliseconds, equal to 1920 samples at 24000 Hz
      encoderSampleRate: 24000,  // 24000 to match model's sample rate
      maxFramesPerPage: 1,
      numberOfChannels: 1,
    });

    recorder.ondataavailable = async (arrayBuffer) => {
      if (socketRef.current) {
        if (socketRef.current.readyState !== WebSocket.OPEN) {
          console.log("Socket not open, dropping audio");
          return;
        }
        await socketRef.current.send(arrayBuffer);
      }
    };

    recorder.start().then(() => {
      console.log("Recording started");
      setRecorder(recorder);
    });

    // create a MediaRecorder object for capturing PCM (calculating amplitude)
    const analyzerContext = new (window.AudioContext || window.webkitAudioContext)();
    const analyzer = analyzerContext.createAnalyser();
    analyzer.fftSize = 256;
    const sourceNode = analyzerContext.createMediaStreamSource(stream);
    sourceNode.connect(analyzer);

    // Use a separate audio processing function instead of MediaRecorder
    const processAudio = () => {
      const dataArray = new Uint8Array(analyzer.frequencyBinCount);
      analyzer.getByteFrequencyData(dataArray);
      const average = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
      setAmplitude(average);
      requestAnimationFrame(processAudio);
    };
    processAudio();
  };


  // Audio Playback: Prep decoder for converting opus to PCM for audio playback
  useEffect(() => {
    const initializeDecoder = async () => {
      const decoder = new window["ogg-opus-decoder"].OggOpusDecoder();
      await decoder.ready;
      decoderRef.current = decoder;
      console.log("Ogg Opus decoder initialized");
    };
  
    initializeDecoder();
  
    return () => {
      if (decoderRef.current) {
        decoderRef.current.free();
      }
    };
  }, []);

  // Audio Playback: schedule PCM audio chunks for seamless playback
  const scheduleAudioPlayback = (newAudioData) => {
    const sampleRate = audioContext.sampleRate;
    const numberOfChannels = 1;
    const nowTime = audioContext.currentTime;
  
    // Create a new buffer and source node for the incoming audio data
    const newBuffer = audioContext.createBuffer(numberOfChannels, newAudioData.length, sampleRate);
    newBuffer.copyToChannel(newAudioData, 0);
    const sourceNode = audioContext.createBufferSource();
    sourceNode.buffer = newBuffer;
    sourceNode.connect(audioContext.destination);
  
    // Schedule the new audio to play immediately after any currently playing audio
    const startTime = Math.max(scheduledEndTimeRef.current, nowTime);
    sourceNode.start(startTime);
  
    // Update the scheduled end time so we know when to schedule the next piece of audio
    scheduledEndTimeRef.current = startTime + newBuffer.duration;
  
    if (sourceNodeRef.current && sourceNodeRef.current.buffer) {
      const currentEndTime = sourceNodeRef.current.startTime + sourceNodeRef.current.buffer.duration;
      if (currentEndTime <= nowTime) {
        sourceNodeRef.current.disconnect();
      }
    }
    sourceNodeRef.current = sourceNode;
  };


  // WebSocket: open websocket connection and start recording
  useEffect(() => {
    const endpoint = getBaseURL();
    console.log("Connecting to", endpoint);
    const socket = new WebSocket(endpoint);
    socketRef.current = socket;

    socket.onopen = () => {
      console.log("WebSocket connection opened");
      startRecording();
      setWarmupComplete(true);
    };

    socket.onmessage = async (event) => {
      // data is a blob, convert to array buffer
      const arrayBuffer = await event.data.arrayBuffer();
      const view = new Uint8Array(arrayBuffer);
      const tag = view[0];
      const payload = arrayBuffer.slice(1);
      if (tag === 1) {
        // audio data
        const { channelData, samplesDecoded, sampleRate } = await decoderRef.current.decode(new Uint8Array(payload));
        if (samplesDecoded > 0) {
          scheduleAudioPlayback(channelData[0]);
        }
      }
      if (tag === 2) {
        // text data
        const decoder = new TextDecoder();
        const text = decoder.decode(payload);

        setPendingSentence(prevPending => {
          const updatedPending = prevPending + text;
          if (updatedPending.endsWith('.') || updatedPending.endsWith('!') || updatedPending.endsWith('?')) {
            setCompletedSentences(prevCompleted => [...prevCompleted, updatedPending]);
            return '';
          }
          return updatedPending;
        });
      }
    };

    socket.onclose = () => {
      console.log("WebSocket connection closed");
    };

    return () => {
      socket.close();
    };
  }, []);

  return (
    <div className="bg-gray-900 text-white min-h-screen flex flex-col items-center justify-center p-4">
      <div className="bg-gray-800 rounded-lg shadow-lg w-full max-w-xl p-6 mb-8">
        <div className="flex">
          <div className="w-5/6 overflow-y-auto max-h-64">
            <TextOutput warmupComplete={warmupComplete} completedSentences={completedSentences} pendingSentence={pendingSentence} />
          </div>
          <div className="w-1/6 ml-4 pl-4">
            <AudioControl recorder={recorder} amplitude={amplitude} />
          </div>
        </div>
      </div>
      
      <a
        className="fixed bottom-4 inline-flex items-center justify-center"
        href="https://modal.com"
        target="_blank"
        rel="noopener noreferrer"
      >
        <footer className="flex items-center px-3 py-2 rounded-lg bg-gray-800 shadow-lg hover:bg-gray-700 transition-colors duration-200">
          <span className="text-sm font-medium text-gray-300 mr-2">
            Built with <a 
              className="underline" 
              href="https://github.com/kyutai-labs/moshi" 
              target="_blank" rel="noopener noreferrer">
                Moshi
            </a> and
          </span>
          <img className="w-24" src="./modal-logo.svg" alt="Modal logo" />
        </footer>
      </a>
    </div>
  );
}

const AudioControl = ({ recorder, amplitude }) => {
  const [muted, setMuted] = useState(true);

  const toggleMute = () => {
    if (!recorder) {
      return;
    }
    setMuted(!muted);
    recorder.setRecordingGain(muted ? 1 : 0);
  };

  // unmute automatically once the recorder is ready
  useEffect(() => {
    if (recorder) {
      setMuted(false);
      recorder.setRecordingGain(1);
    }
  },
  [recorder]);

  const amplitudePercent = amplitude / 255;
  const maxAmplitude = 0.3; // for scaling
  const minDiameter = 30; // minimum diameter of the circle in pixels
  const maxDiameter = 200; // increased maximum diameter to ensure overflow
  
  var diameter = minDiameter + (maxDiameter - minDiameter) * (amplitudePercent / maxAmplitude);
  if (muted) {
    diameter = 20;
  }

  return (
    <div className="w-full h-full flex items-center">
      <div className="w-full h-6 rounded-sm relative overflow-hidden">
        <div className="absolute inset-0 flex items-center justify-center">
          <div
            className={`rounded-full transition-all duration-100 ease-out hover:cursor-pointer ${muted ? 'bg-gray-200 hover:bg-red-300' : 'bg-red-500 hover:bg-red-300'}`}
            onClick={toggleMute}
            style={{
              width: `${diameter}px`,
              height: `${diameter}px`,
            }}
          ></div>
        </div>
      </div>
    </div>
  );
};

const TextOutput = ({ warmupComplete, completedSentences, pendingSentence }) => {
  const containerRef = useRef(null);
  const allSentences = [...completedSentences, pendingSentence];
  if (pendingSentence.length === 0 && allSentences.length > 1) {
    allSentences.pop();
  }

  useEffect(() => {
    if (containerRef.current) {
      containerRef.current.scrollTop = containerRef.current.scrollHeight;
    }
  }, [completedSentences, pendingSentence]);

  return (
    <div ref={containerRef} className="flex flex-col-reverse overflow-y-auto max-h-64 pr-2">
      {warmupComplete ? (
        allSentences.map((sentence, index) => (
          <p key={index} className="text-gray-300 my-2">{sentence}</p>
        )).reverse()
      ) : (
        <p className="text-gray-400 animate-pulse">Warming up model...</p>
      )}
    </div>
  );
};



const container = document.getElementById("react");
ReactDOM.createRoot(container).render(<App />);

================
File: src/frontend/index.html
================
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Modal Chat</title>
    <link
      href="https://fonts.googleapis.com/css?family=Inter:300,400,600"
      rel="stylesheet"
    />
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://unpkg.com/react@18/umd/react.development.js"></script>
    <script src="https://unpkg.com/react-dom@18/umd/react-dom.development.js"></script>
    <script src="https://unpkg.com/@babel/standalone/babel.min.js"></script>

    <!-- Opus recorder -->
    <script src="https://cdn.jsdelivr.net/npm/opus-recorder@latest/dist/recorder.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/opus-recorder@latest/dist/encoderWorker.min.js"></script>

    <!-- Opus decoder -->
    <script src="https://cdn.jsdelivr.net/npm/ogg-opus-decoder/dist/ogg-opus-decoder.min.js"></script>
      
    <script>
      tailwind.config = {
        theme: {
          extend: {
            colors: {
              // Dark base background color
              ground: "#0C0F0B",

              // Theme colors
              primary: "#9AEE86",
              "accent-pink": "#FC9CC6",
              "accent-blue": "#B8E4FF",
            },
          },
        },
      };
    </script>
  </head>

  <body class="bg-zinc-700">
    <noscript>You must have JavaScript enabled to use this app.</noscript>
    <script data-type="module" type="text/babel" src="/app.jsx"></script>
    <div id="react"></div>
  </body>
</html>

================
File: src/frontend/modal-logo.svg
================
<svg width="500" height="140" viewBox="0 0 500 140" fill="none" xmlns="http://www.w3.org/2000/svg">
<path fill-rule="evenodd" clip-rule="evenodd" d="M58.1127 21.9828H95.5097L113.78 54.1729L77.9737 117.041H40.5705L21.8832 84.4475L58.1127 21.9828ZM59.6426 30.1081L28.1167 84.4631L42.127 108.899L73.2984 54.1683L59.6426 30.1081ZM77.9737 56.8706L46.7797 111.641H74.8348L106.029 56.8706H77.9737ZM106.037 51.4706H77.9764L64.305 27.3828H92.3654L106.037 51.4706Z" fill="#C5FFB8"/>
<path fill-rule="evenodd" clip-rule="evenodd" d="M199.662 84.8558L180.963 117.243L143.951 116.97L107.408 54.5266L126.11 22.1345L163.681 22.2478L199.662 84.8558ZM191.86 82.1181L160.55 27.6384L132.383 27.5534L164.196 81.9143L191.86 82.1181ZM159.518 84.6121L127.682 30.2118L113.654 54.5083L145.49 108.909L159.518 84.6121ZM150.163 111.615L164.193 87.3144L191.889 87.5185L177.859 111.819L150.163 111.615Z" fill="#C5FFB8"/>
<path d="M241.56 41.36H256.92L272.04 81.04L287.32 41.36H302.52V98H292.92V50.96H292.76L275.56 98H268.52L251.32 50.96H251.16V98H241.56V41.36ZM312.583 78.8C312.583 75.76 313.116 73.0133 314.183 70.56C315.303 68.0533 316.796 65.92 318.663 64.16C320.529 62.4 322.743 61.04 325.303 60.08C327.863 59.12 330.583 58.64 333.463 58.64C336.343 58.64 339.063 59.12 341.623 60.08C344.183 61.04 346.396 62.4 348.263 64.16C350.129 65.92 351.596 68.0533 352.663 70.56C353.783 73.0133 354.343 75.76 354.343 78.8C354.343 81.84 353.783 84.6133 352.663 87.12C351.596 89.5733 350.129 91.68 348.263 93.44C346.396 95.2 344.183 96.56 341.623 97.52C339.063 98.48 336.343 98.96 333.463 98.96C330.583 98.96 327.863 98.48 325.303 97.52C322.743 96.56 320.529 95.2 318.663 93.44C316.796 91.68 315.303 89.5733 314.183 87.12C313.116 84.6133 312.583 81.84 312.583 78.8ZM322.183 78.8C322.183 80.2933 322.423 81.7333 322.903 83.12C323.436 84.5067 324.183 85.7333 325.143 86.8C326.156 87.8667 327.356 88.72 328.743 89.36C330.129 90 331.703 90.32 333.463 90.32C335.223 90.32 336.796 90 338.183 89.36C339.569 88.72 340.743 87.8667 341.703 86.8C342.716 85.7333 343.463 84.5067 343.943 83.12C344.476 81.7333 344.743 80.2933 344.743 78.8C344.743 77.3067 344.476 75.8667 343.943 74.48C343.463 73.0933 342.716 71.8667 341.703 70.8C340.743 69.7333 339.569 68.88 338.183 68.24C336.796 67.6 335.223 67.28 333.463 67.28C331.703 67.28 330.129 67.6 328.743 68.24C327.356 68.88 326.156 69.7333 325.143 70.8C324.183 71.8667 323.436 73.0933 322.903 74.48C322.423 75.8667 322.183 77.3067 322.183 78.8ZM393.731 92.24H393.571C392.184 94.5867 390.317 96.2933 387.971 97.36C385.624 98.4267 383.091 98.96 380.371 98.96C377.384 98.96 374.717 98.4533 372.371 97.44C370.077 96.3733 368.104 94.9333 366.451 93.12C364.797 91.3067 363.544 89.1733 362.691 86.72C361.837 84.2667 361.411 81.6267 361.411 78.8C361.411 75.9733 361.864 73.3333 362.771 70.88C363.677 68.4267 364.931 66.2933 366.531 64.48C368.184 62.6667 370.157 61.2533 372.451 60.24C374.744 59.1733 377.251 58.64 379.971 58.64C381.784 58.64 383.384 58.8267 384.771 59.2C386.157 59.5733 387.384 60.0533 388.451 60.64C389.517 61.2267 390.424 61.8667 391.171 62.56C391.917 63.2 392.531 63.84 393.011 64.48H393.251V37.52H402.851V98H393.731V92.24ZM371.011 78.8C371.011 80.2933 371.251 81.7333 371.731 83.12C372.264 84.5067 373.011 85.7333 373.971 86.8C374.984 87.8667 376.184 88.72 377.571 89.36C378.957 90 380.531 90.32 382.291 90.32C384.051 90.32 385.624 90 387.011 89.36C388.397 88.72 389.571 87.8667 390.531 86.8C391.544 85.7333 392.291 84.5067 392.771 83.12C393.304 81.7333 393.571 80.2933 393.571 78.8C393.571 77.3067 393.304 75.8667 392.771 74.48C392.291 73.0933 391.544 71.8667 390.531 70.8C389.571 69.7333 388.397 68.88 387.011 68.24C385.624 67.6 384.051 67.28 382.291 67.28C380.531 67.28 378.957 67.6 377.571 68.24C376.184 68.88 374.984 69.7333 373.971 70.8C373.011 71.8667 372.264 73.0933 371.731 74.48C371.251 75.8667 371.011 77.3067 371.011 78.8ZM437.721 92.72H437.481C436.095 94.9067 434.335 96.5067 432.201 97.52C430.068 98.48 427.721 98.96 425.161 98.96C423.401 98.96 421.668 98.72 419.961 98.24C418.308 97.76 416.815 97.04 415.481 96.08C414.201 95.12 413.161 93.92 412.361 92.48C411.561 91.04 411.161 89.36 411.161 87.44C411.161 85.36 411.535 83.6 412.281 82.16C413.028 80.6667 414.015 79.44 415.241 78.48C416.521 77.4667 417.988 76.6667 419.641 76.08C421.295 75.4933 423.001 75.0667 424.761 74.8C426.575 74.48 428.388 74.2933 430.201 74.24C432.015 74.1333 433.721 74.08 435.321 74.08H437.721V73.04C437.721 70.64 436.895 68.8533 435.241 67.68C433.588 66.4533 431.481 65.84 428.921 65.84C426.895 65.84 425.001 66.2133 423.241 66.96C421.481 67.6533 419.961 68.6133 418.681 69.84L413.641 64.8C415.775 62.6133 418.255 61.04 421.081 60.08C423.961 59.12 426.921 58.64 429.961 58.64C432.681 58.64 434.975 58.96 436.841 59.6C438.708 60.1867 440.255 60.96 441.481 61.92C442.708 62.88 443.641 64 444.281 65.28C444.975 66.5067 445.455 67.76 445.721 69.04C446.041 70.32 446.228 71.5733 446.281 72.8C446.335 73.9733 446.361 75.0133 446.361 75.92V98H437.721V92.72ZM437.161 80.8H435.161C433.828 80.8 432.335 80.8533 430.681 80.96C429.028 81.0667 427.455 81.3333 425.961 81.76C424.521 82.1333 423.295 82.72 422.281 83.52C421.268 84.2667 420.761 85.3333 420.761 86.72C420.761 87.6267 420.948 88.4 421.321 89.04C421.748 89.6267 422.281 90.1333 422.921 90.56C423.561 90.9867 424.281 91.3067 425.081 91.52C425.881 91.68 426.681 91.76 427.481 91.76C430.788 91.76 433.215 90.9867 434.761 89.44C436.361 87.84 437.161 85.68 437.161 82.96V80.8ZM456.85 37.52H466.45V98H456.85V37.52Z" fill="white"/>
</svg>

================
File: src/moshi.py
================
"""
Moshi websocket web service.
"""

import modal
import asyncio
import time

from .common import app

image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "moshi==0.1.0",
        "fastapi==0.115.5",
        "huggingface_hub==0.24.7",
        "hf_transfer==0.1.8",
        "sphn==0.1.4",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
)

with image.imports():
    from huggingface_hub import hf_hub_download
    import torch
    from moshi.models import loaders, LMGen
    import sentencepiece
    import sphn
    import numpy as np


@app.cls(
    image=image,
    gpu="A10G",
    container_idle_timeout=300,
    timeout=600,
)
class Moshi:
    @modal.build()
    def download_model(self):
        hf_hub_download(loaders.DEFAULT_REPO, loaders.MOSHI_NAME)
        hf_hub_download(loaders.DEFAULT_REPO, loaders.MIMI_NAME)
        hf_hub_download(loaders.DEFAULT_REPO, loaders.TEXT_TOKENIZER_NAME)

    @modal.enter()
    def enter(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        mimi_weight = hf_hub_download(loaders.DEFAULT_REPO, loaders.MIMI_NAME)
        self.mimi = loaders.get_mimi(mimi_weight, device=self.device)
        self.mimi.set_num_codebooks(8)
        self.frame_size = int(self.mimi.sample_rate / self.mimi.frame_rate)

        moshi_weight = hf_hub_download(loaders.DEFAULT_REPO, loaders.MOSHI_NAME)
        self.moshi = loaders.get_moshi_lm(moshi_weight, device=self.device)
        self.lm_gen = LMGen(
            self.moshi,
            # Sampling params
            temp=0.8,
            temp_text=0.8,
            top_k=250,
            top_k_text=25,
        )

        self.mimi.streaming_forever(1)
        self.lm_gen.streaming_forever(1)

        tokenizer_config = hf_hub_download(
            loaders.DEFAULT_REPO, loaders.TEXT_TOKENIZER_NAME
        )
        self.text_tokenizer = sentencepiece.SentencePieceProcessor(tokenizer_config)

        # Warmup them GPUs
        for chunk in range(4):
            chunk = torch.zeros(
                1, 1, self.frame_size, dtype=torch.float32, device=self.device
            )
            codes = self.mimi.encode(chunk)
            for c in range(codes.shape[-1]):
                tokens = self.lm_gen.step(codes[:, :, c : c + 1])
                if tokens is None:
                    continue
                _ = self.mimi.decode(tokens[:, 1:])
        torch.cuda.synchronize()

    def reset_state(self):
        # we use Opus format for audio across the websocket, as it can be safely streamed and decoded in real-time
        self.opus_stream_outbound = sphn.OpusStreamWriter(self.mimi.sample_rate)
        self.opus_stream_inbound = sphn.OpusStreamReader(self.mimi.sample_rate)

        # LLM is stateful, maintaining chat history, so reset it on each connection
        self.mimi.reset_streaming()
        self.lm_gen.reset_streaming()

    @modal.asgi_app()
    def web(self):
        from fastapi import FastAPI, Response, WebSocket, WebSocketDisconnect

        web_app = FastAPI()

        @web_app.get("/status")
        async def status():
            return Response(status_code=200)

        @web_app.websocket("/ws")
        async def websocket(ws: WebSocket):
            with torch.no_grad():
                await ws.accept()

                # Clear model chat history and any buffered audio
                self.reset_state()

                print("Session started")
                tasks = []

                # We use asyncio to run multiple loops concurrently, within the context of this single websocket connection
                async def recv_loop():
                    """
                    Receives Opus stream across websocket, appends into opus_stream_inboun
                    """
                    while True:
                        data = await ws.receive_bytes()

                        if not isinstance(data, bytes):
                            print("received non-bytes message")
                            continue
                        if len(data) == 0:
                            print("received empty message")
                            continue
                        self.opus_stream_inbound.append_bytes(data)

                async def inference_loop():
                    """
                    Runs streaming inference on inbound data, and if any response audio is created, appends it to the outbound stream
                    """
                    all_pcm_data = None
                    while True:
                        await asyncio.sleep(0.001)
                        pcm = self.opus_stream_inbound.read_pcm()
                        if pcm is None:
                            continue
                        if len(pcm) == 0:
                            continue

                        if pcm.shape[-1] == 0:
                            continue
                        if all_pcm_data is None:
                            all_pcm_data = pcm
                        else:
                            all_pcm_data = np.concatenate((all_pcm_data, pcm))

                        # infer on each frame
                        while all_pcm_data.shape[-1] >= self.frame_size:
                            t0 = time.time()

                            chunk = all_pcm_data[: self.frame_size]
                            all_pcm_data = all_pcm_data[self.frame_size :]

                            chunk = torch.from_numpy(chunk)
                            chunk = chunk.to(device=self.device)[None, None]

                            # inference on audio chunk
                            codes = self.mimi.encode(chunk)

                            # language model inference against encoded audio
                            for c in range(codes.shape[-1]):
                                tokens = self.lm_gen.step(codes[:, :, c : c + 1])

                                if tokens is None:
                                    # model is silent
                                    continue

                                assert tokens.shape[1] == self.lm_gen.lm_model.dep_q + 1
                                main_pcm = self.mimi.decode(tokens[:, 1:])
                                main_pcm = main_pcm.cpu()
                                self.opus_stream_outbound.append_pcm(
                                    main_pcm[0, 0].numpy()
                                )

                                text_token = tokens[0, 0, 0].item()
                                if text_token not in (0, 3):
                                    text = self.text_tokenizer.id_to_piece(text_token)
                                    text = text.replace("▁", " ")
                                    msg = b"\x02" + bytes(
                                        text, encoding="utf8"
                                    )  # prepend "\x02" as a tag to indicate text
                                    await ws.send_bytes(msg)

                async def send_loop():
                    """
                    Reads outbound data, and sends it across websocket
                    """
                    while True:
                        await asyncio.sleep(0.001)
                        msg = self.opus_stream_outbound.read_bytes()
                        if msg is None:
                            continue
                        if len(msg) == 0:
                            continue
                        msg = b"\x01" + msg  # prepend "\x01" as a tag to indicate audio
                        await ws.send_bytes(msg)

                # This runs all the loops concurrently
                try:
                    tasks = [
                        asyncio.create_task(recv_loop()),
                        asyncio.create_task(inference_loop()),
                        asyncio.create_task(send_loop()),
                    ]
                    await asyncio.gather(*tasks)

                except WebSocketDisconnect:
                    print("WebSocket disconnected")
                    await ws.close(code=1000)
                except Exception as e:
                    print("Exception:", e)
                    await ws.close(code=1011)  # Internal error
                    raise e
                finally:
                    for task in tasks:
                        task.cancel()
                    await asyncio.gather(*tasks, return_exceptions=True)
                    # self.opus_stream_inbound.close()
                    self.reset_state()

        return web_app

================
File: tests/e2e_test.py
================
# For CI smoke testing only
# Assumes serve endpoint running at modal-labs--quillman-moshi-web-dev.modal.run

import asyncio
from pathlib import Path

import os
import aiohttp
import time

endpoint = "wss://modal-labs--quillman-moshi-web-dev.modal.run/"

shutdown_flag = asyncio.Event()

WARMUP_TIMEOUT = 60 # give server 60 seconds to warm up
async def ensure_server_ready():
    deadline = time.time() + WARMUP_TIMEOUT
    async with aiohttp.ClientSession() as session:
        while time.time() < deadline:
            try:
                print("Checking server status...")
                resp = await session.get(endpoint + "status")
                if resp.status == 200:
                    return
            except Exception as e:
                print("Error while checking server status:", e)
                await asyncio.sleep(5)
                pass

async def run():
    await ensure_server_ready()

    send_chunks = []
    recv_chunks = []

    files = os.listdir(Path(__file__).parent / "e2e_in")
    files.sort()
    for chunk_file in files:
        with open(Path(__file__).parent / "e2e_in" / chunk_file, "rb") as f:
            data = f.read()
            send_chunks.append(data)

    print("Connecting to", endpoint + "ws")
    async with aiohttp.ClientSession() as session:
        async with session.ws_connect(endpoint + "ws") as ws:
            print("Connection established.")
            
            async def send_loop():
                for chunk in send_chunks:
                    await asyncio.sleep(0.1)
                    await ws.send_bytes(chunk)
                    print("Sent chunk, len:", len(chunk))

            async def recv_loop():
                async for msg in ws:
                    if shutdown_flag.is_set():
                        break
                    data = msg.data
                    if not isinstance(data, bytes) or len(data) == 0:
                        continue
                    print("Received chunk, len:", len(data))
                    recv_chunks.append(data)

            async def timeout_loop():
                await asyncio.sleep(10)
                shutdown_flag.set()
                await ws.close()
        
            await asyncio.gather(send_loop(), recv_loop(), timeout_loop())

    assert(len(recv_chunks) >= 3) # Opus sends two headers always, so at least three should be received for healthy inference
    print("Done")

if __name__ == "__main__":
    asyncio.run(run())

================
File: tests/moshi_client.py
================
import asyncio
import queue
import sys
import signal
import subprocess

# External dependencies
import sphn
import aiohttp
import sounddevice as sd
import numpy as np

profile = subprocess.run(
    ["modal", "profile", "current"], check=True, capture_output=True, text=True
).stdout.splitlines()[0]

app_name = "quillman"
class_name = "moshi"

endpoint = f"wss://{profile}--{app_name}-{class_name}-web-dev.modal.run/ws"

# Global flag for shutdown
shutdown_flag = asyncio.Event()

# ANSI escape codes for colored text
GREEN = '\033[92m'
RESET = '\033[0m'

# Connection manager
class Connection:
    def __init__(self, ws):
        self.ws = ws
        self.sample_rate = 24000
        self.frame_size = 1920
        self.channels = 1

        # The Opus audio codec is used for streaming audio to the websocket
        # For spoken voice, it can compress as much as 10x, and can be decoded in real-time
        self.opus_writer = sphn.OpusStreamWriter(self.sample_rate)
        self.opus_reader = sphn.OpusStreamReader(self.sample_rate)

        self.audio_in_stream = sd.InputStream(
            samplerate=self.sample_rate,
            channels=self.channels,
            blocksize=self.frame_size,
            callback=self.audio_in_callback,
        )

        self.audio_out_stream = sd.OutputStream(
            samplerate=self.sample_rate,
            channels=self.channels,
            blocksize=self.frame_size,
            callback=self.audio_out_callback,
        )     
        self.out_queue = queue.Queue()

    # Sounddevice callbacks for handling raw audio to/from the speaker and mic
    def audio_in_callback(self, data, frames, time, status):
        self.opus_writer.append_pcm(data[:, 0])

    def audio_out_callback(self, data, frames, time, status):
        try:
            pcm_data = self.out_queue.get(block=False)
            assert pcm_data.shape == (self.frame_size,), pcm_data.shape
            data[:, 0] = pcm_data
        except queue.Empty:
            data.fill(0)

    

    # Async loops for bidirectional audio streaming:

    async def send_loop(self):
        '''
        Async loop for sending opus stream to the websocket
        '''
        while not shutdown_flag.is_set():
            await asyncio.sleep(0.001)
            msg = self.opus_writer.read_bytes()
            if len(msg) > 0:
                try:
                    await self.ws.send_bytes(msg)
                except Exception as e:
                    print(f"Error in send_loop: {e}")
                    return
                
    async def receive_loop(self):
        '''
        Async loop for receiving messages from the websocket, including text and opus stream
        '''
        sentence = ""
        try:
            async for msg in self.ws:
                if shutdown_flag.is_set():
                    break
                msg_bytes = msg.data
                if not isinstance(msg_bytes, bytes) or len(msg_bytes) == 0:
                    continue

                # First byte in message is a tag, indicating audio or text.
                tag = msg_bytes[0]
                payload = msg_bytes[1:]
                
                if tag == 1:
                    # payload is opus audio
                    self.opus_reader.append_bytes(payload)
                
                elif tag == 2:
                    # payload is text output from the model, print it to the console
                    token = payload.decode("utf8")
                    sentence += token
                    sys.stdout.write(f"\r{GREEN}{sentence.lstrip()}{RESET}")
                    sys.stdout.flush()
                    if sentence.strip()[-1] in [".", "!", "?"]:
                        sys.stdout.write("\n")
                        sentence = ""

        except Exception as e:
            print(f"Error in receive_loop: {e}")
        
    async def decoder_loop(self):
        '''
        Async loop for decoding audio from the websocket into raw pcm audio, and queueing it for playback
        '''

        all_pcm_data = None
        while not shutdown_flag.is_set():
            await asyncio.sleep(0.001)
            pcm = self.opus_reader.read_pcm()
            if all_pcm_data is None:
                all_pcm_data = pcm
            else:
                all_pcm_data = np.concatenate((all_pcm_data, pcm))
            while all_pcm_data.shape[-1] >= self.frame_size:
                self.out_queue.put(all_pcm_data[: self.frame_size])
                all_pcm_data = np.array(all_pcm_data[self.frame_size :])


    async def run(self):
        try:
            with self.audio_in_stream, self.audio_out_stream:
                self.futures = asyncio.gather(
                    self.send_loop(), self.receive_loop(), self.decoder_loop()
                )
                await self.futures
        except asyncio.CancelledError:
            print("Connection tasks cancelled")

# Handle keyboard interrupts
def sigint_handler(signum, frame):
    print("\n\nEnding conversation...")
    shutdown_flag.set()

async def run():
    signal.signal(signal.SIGINT, sigint_handler)

    print("Connecting to", endpoint)
    print("This may trigger a cold boot of the model...\n")
    async with aiohttp.ClientSession() as session:
        try:
            async with session.ws_connect(endpoint) as ws:
                connection = Connection(ws)
                print("Connection established.")
                print("Conversation started. Press Ctrl+C to exit.\n")
                await connection.run()
        except aiohttp.ClientError as e:
            print(f"Connection error: {e}")

if __name__ == "__main__":
    try:
        asyncio.run(run())
    except KeyboardInterrupt:
        print("\nProgram interrupted")
    finally:
        print("Conversation complete")
